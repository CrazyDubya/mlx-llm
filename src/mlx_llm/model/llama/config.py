llama_config = {
    "llama-7B": {
        "dim": 4096,
        "mlp_dim": 1108,
        "n_heads": 32,
        "n_layers": 32,
        "vocab_size": 32000,
    }
}